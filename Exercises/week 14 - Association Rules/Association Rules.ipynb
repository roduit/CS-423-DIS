{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will implement two techniques that are part of the so-called shopping basket analysis, which will help us to better understand how customers data are being processed to extract insights about their habits.\n",
    "\n",
    "\n",
    "#### Notes about external libraries\n",
    "You can check your implementation of the Apriori algorithm and the Association Rules using MLxtend, a data mining library. Unfortunately, the library is not directly shipped with Anaconda. To install MLxtend, just execute  \n",
    "\n",
    "```bash\n",
    "pip install mlxtend  \n",
    "```\n",
    "\n",
    "Or directly using Anaconda\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge mlxtend \n",
    "```\n",
    "\n",
    "Note that the installation of MLxtend is not mandatory, as we will provide the expected results in pre-rendered cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 1: Apriori algorithm\n",
    "In the first excercise, we will put into practice the Apriori algorithm. In particular, we will extract frequent itemsets from a list of transactions coming from a grocery store. You will have to complete the function `get_support(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(dataset):\n",
    "    \"\"\"\n",
    "    Format the transaction dataset.\n",
    "    Expect a list of transaction in the format:\n",
    "    [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], ...]\n",
    "    \"\"\"\n",
    "    unique_items = set()\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            unique_items.add(item)\n",
    "       \n",
    "    # Converting to frozensets to use itemsets as dict key\n",
    "    unique_items = [frozenset([i]) for i in list(unique_items)]\n",
    "    \n",
    "    return unique_items,list(map(set,dataset))\n",
    "\n",
    "\n",
    "def generate_candidates(Lk):\n",
    "    \"\"\"\n",
    "    Generate candidates of length n+1 from a list of items, each of length n.\n",
    "\n",
    "    Example:\n",
    "    [{1}, {2}, {5}]          -> [{1, 2}, {1, 5}, {2, 5}]\n",
    "    [{2, 3}, {2, 5}, {3, 5}] -> [{2, 3, 5}]\n",
    "    \"\"\"\n",
    "    output = []\n",
    "\n",
    "    # We generate rules of the target size k\n",
    "    k=len(Lk[0])+1\n",
    "    \n",
    "    for i in range(len(Lk)):\n",
    "        for j in range(i+1, len(Lk)): \n",
    "            L1 = list(Lk[i])[:k-2]; \n",
    "            L2 = list(Lk[j])[:k-2]\n",
    "            L1.sort(); \n",
    "            L2.sort()\n",
    "\n",
    "            # Merge sets if first k-2 elements are equal\n",
    "            # For the case of k<2, generate all possible combinations\n",
    "            if L1==L2: \n",
    "                output.append(Lk[i] | Lk[j])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def print_support(support,max_display=10,min_items=1):\n",
    "    \"\"\"\n",
    "    Print the results of the apriori algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    print('support\\t itemset')\n",
    "    print('-'*30)\n",
    "    filt_support = {k:v for k,v in support.items() if len(k)>=min_items}\n",
    "    for s,sup in sorted(filt_support.items(), key=operator.itemgetter(1),reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup,'\\t',set(s))\n",
    "        \n",
    "def print_support_mx(df,max_display=10,min_items=1):\n",
    "    print('support\\t itemset')\n",
    "    print('-'*30)\n",
    "    lenrow = df['itemsets'].apply(lambda x: len(x))\n",
    "    df  = df[lenrow>=min_items]\n",
    "    df  = df.sort_values('support',ascending=False).iloc[:max_display]\n",
    "    for i,row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['support']),'\\t',set(row['itemsets']))\n",
    "        \n",
    "\n",
    "def apriori(dataset, min_support = 0.5):\n",
    "    \"\"\"\n",
    "    Run the apriori algorithm\n",
    "\n",
    "    dataset     : list of transactions\n",
    "    min_support : minimum support. Itemsets with support below this threshold\n",
    "                will be pruned.\n",
    "    \"\"\"\n",
    "    unique_items,dataset = preprocess(dataset)\n",
    "    L1, supportData      = get_support(dataset, unique_items, min_support)\n",
    "    \n",
    "    L = [L1]\n",
    "    k = 0\n",
    "    while True:\n",
    "        Ck       = generate_candidates(L[k])\n",
    "        Lk, supK = get_support(dataset, Ck, min_support)\n",
    "        \n",
    "        # Is there itemsets of length k that have the minimum support ?\n",
    "        if len(Lk)>0:\n",
    "            supportData.update(supK)\n",
    "            L.append(Lk) \n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "The [Apriori Algorithm](https://en.wikipedia.org/wiki/Apriori_algorithm) identifies frequent combinations of items by extending them to larger and larger itemsets (see the generate_candidates function) as long as they appear sufficiently often in a list of transactions.\n",
    "\n",
    "Compute support for all the candidate itemsets contained in Ck, given the total list of transactions. We already provide the functions to compute candidate itemsets. The support of the itemset $X$ with respect to the list of transactions $T$ is defined as the proportion of transactions $t$ in the dataset which contains the itemset $X$. Support can be computed using the following formula\n",
    "\n",
    "$$\\mathrm{supp}(X) = \\frac{|\\{t \\in T; X \\subseteq t\\}|}{|T|}$$  \n",
    "\n",
    "After computing the support for each itemset, prune the ones that do not match the minimal specificied support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_support(dataset, Ck, min_support):\n",
    "    \"\"\"\n",
    "    Compute support for each provided itemset by counting the number of\n",
    "    its occurences in the original dataset of transactions.\n",
    "\n",
    "    dataset      : list of transactions, preprocessed using 'preprocess()'\n",
    "    Ck           : list of itemsets to compute support for. \n",
    "    min_support  : minimum support. Itemsets with support below this threshold\n",
    "                will be pruned.\n",
    "                \n",
    "    output       : list of remaining itemsets, after the pruning step.\n",
    "    support_dict : dictionary containing the support value for each itemset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This dictionary should contain the number of appearance of each itemset in the dataset.\n",
    "    # Itemset in Ck are represented as frozensets and can directly be uses as dictionary keys.\n",
    "    support_count = {}\n",
    "    \n",
    "    for transaction in dataset:\n",
    "        for candidate in Ck:\n",
    "            if candidate.issubset(transaction):\n",
    "                if candidate in support_count.keys():\n",
    "                    support_count[candidate] += 1\n",
    "                else:\n",
    "                    support_count[candidate] = 1\n",
    "\n",
    "    output = []\n",
    "    support_dict = {}\n",
    "    for key in support_count:\n",
    "        \n",
    "        support = support_count[key] / len(dataset)\n",
    "        \n",
    "        if support >= min_support:\n",
    "            output.insert(0,key)\n",
    "            support_dict[key] = support\n",
    "    return output, support_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.07 \t {'whole milk', 'other vegetables'}\n",
      "0.06 \t {'whole milk', 'rolls/buns'}\n",
      "0.06 \t {'yogurt', 'whole milk'}\n",
      "0.05 \t {'root vegetables', 'whole milk'}\n",
      "0.05 \t {'root vegetables', 'other vegetables'}\n",
      "0.04 \t {'yogurt', 'other vegetables'}\n",
      "0.04 \t {'rolls/buns', 'other vegetables'}\n",
      "0.04 \t {'whole milk', 'tropical fruit'}\n",
      "0.04 \t {'whole milk', 'soda'}\n",
      "0.04 \t {'rolls/buns', 'soda'}\n"
     ]
    }
   ],
   "source": [
    "dataset = [ l.strip().split(',') for i,l in enumerate(open('groceries.csv').readlines())]\n",
    "\n",
    "L,support = apriori(dataset,min_support=0.01)\n",
    "print_support(support,10,min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "\n",
    "You can check the results of your implementation using MLXtend. Just run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/dis/lib/python3.9/site-packages/mlxtend/frequent_patterns/fpcommon.py:161: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.07 \t {'whole milk', 'other vegetables'}\n",
      "0.06 \t {'whole milk', 'rolls/buns'}\n",
      "0.06 \t {'yogurt', 'whole milk'}\n",
      "0.05 \t {'root vegetables', 'whole milk'}\n",
      "0.05 \t {'root vegetables', 'other vegetables'}\n",
      "0.04 \t {'yogurt', 'other vegetables'}\n",
      "0.04 \t {'rolls/buns', 'other vegetables'}\n",
      "0.04 \t {'whole milk', 'tropical fruit'}\n",
      "0.04 \t {'whole milk', 'soda'}\n",
      "0.04 \t {'soda', 'rolls/buns'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori as mx_apriori\n",
    "\n",
    "# Assuming `dataset` is defined as a list of lists or similar\n",
    "df_dummy = pd.get_dummies(pd.Series(dataset).apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "frequent_itemsets = mx_apriori(df_dummy, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Assuming `print_support_mx` is a custom function\n",
    "print_support_mx(frequent_itemsets, 10, min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 2: Association Rule Learning\n",
    "Such associations are not necessarily symmetric. Therefore, in the second part, we will use [association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning) to better understand the directionality of our computed frequent itemsets. In other terms, we will have to infer if the purchase of one item generally implies the the purchase of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(L, supportData, min_confidence=0.7):  \n",
    "    \"\"\"\n",
    "    L              : itemsets\n",
    "    supportData    : dictionary storing itemsets support\n",
    "    min_confidence : rules with a confidence under this threshold should be pruned\n",
    "    \"\"\"\n",
    "    # Rules to be computed\n",
    "    rules = []\n",
    "    \n",
    "    # Iterate over itemsets of length 2..N\n",
    "    for i in range(1, len(L)):\n",
    "        \n",
    "        # Iterate over each frequent itemset\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            \n",
    "            # If the itemset contains more than 2 elements\n",
    "            # recursively generate candidates \n",
    "            if (i+1 > 2):\n",
    "                rules_from_consequent(freqSet, H1, supportData, rules, min_confidence)\n",
    "                compute_confidence(freqSet, H1, supportData, rules, min_confidence)\n",
    "            # If the itemsset contains 2 or less elements\n",
    "            # conpute rule confidence\n",
    "            else:\n",
    "                compute_confidence(freqSet, H1, supportData, rules, min_confidence)\n",
    "\n",
    "    return rules   \n",
    "\n",
    "def rules_from_consequent(freqSet, H, supportData, rules, min_confidence=0.7):\n",
    "    \"\"\"\n",
    "    freqSet        : frequent itemset\n",
    "    H              : candidate elements to create a rule\n",
    "    supportData    : dictionary storing itemsets support\n",
    "    rules          : array to store rules\n",
    "    min_confidence : rules with a confidence under this threshold should be pruned\n",
    "    \"\"\"\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)): \n",
    "\n",
    "        # create new candidates of size n+1\n",
    "        Hmp1 = generate_candidates(H)\n",
    "        Hmp1 = compute_confidence(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "        \n",
    "        if (len(Hmp1) > 1):    #need at least two sets to merge\n",
    "            rules_from_consequent(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "            \n",
    "\n",
    "def print_rules(rules,max_display=10):\n",
    "    \"\"\"\n",
    "    Print the resulting rules\n",
    "    \"\"\"\n",
    "    print('confidence\\t rule')\n",
    "    print('-'*30)\n",
    "    for a,b,sup in sorted(rules, key=lambda x: x[2],reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup,'\\t',set(a),'->',set(b))\n",
    "def print_rules_mx(df,max_display=10):\n",
    "    print('confidence\\t rule')\n",
    "    print('-'*30)\n",
    "    df  = df.sort_values('confidence',ascending=False).iloc[:max_display]\n",
    "    for i,row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['confidence']),'\\t',set(row['antecedents']),'->',set(row['consequents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "You will have to complete the method `compute_confidence(...)`, that computes confidence for a set of candidate rules H and prunes the rules that have a confidence below the specified threshold. Please complete it by computing rules confidence using the following formula:\n",
    "\n",
    "$$\\mathrm{conf}(X \\Rightarrow Y) = \\mathrm{supp}(X \\cup Y) / \\mathrm{supp}(X)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence(freqSet, H, supportData, rules, min_confidence=0.7):\n",
    "    \"\"\"\n",
    "    Compute confidence for a given set of rules and their respective support\n",
    "\n",
    "    freqSet        : frequent itemset of N-element\n",
    "    H              : list of candidate elements Y1, Y2... that are part of the frequent itemset\n",
    "    supportData    : dictionary storing itemsets support\n",
    "    rules          : array to store rules\n",
    "    min_confidence : rules with a confidence under this threshold should be pruned\n",
    "    \"\"\"\n",
    "    prunedH = [] \n",
    "    \n",
    "    for Y in H:\n",
    "        # Compute X which is the frequent itemset minus the considered Y\n",
    "        X           = freqSet - Y\n",
    "        \n",
    "        # Compute support for both terms\n",
    "        support_XuY = supportData[freqSet]\n",
    "        support_X   = supportData[X]\n",
    "        \n",
    "        # Compute confidence\n",
    "        conf        = support_XuY / support_X\n",
    "        \n",
    "        if conf >= min_confidence: \n",
    "            rules.append((X, Y, conf))\n",
    "            prunedH.append(Y)\n",
    "    return prunedH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "0.59 \t {'citrus fruit', 'root vegetables'} -> {'other vegetables'}\n",
      "0.58 \t {'root vegetables', 'tropical fruit'} -> {'other vegetables'}\n",
      "0.58 \t {'yogurt', 'curd'} -> {'whole milk'}\n",
      "0.57 \t {'butter', 'other vegetables'} -> {'whole milk'}\n",
      "0.57 \t {'root vegetables', 'tropical fruit'} -> {'whole milk'}\n",
      "0.56 \t {'yogurt', 'root vegetables'} -> {'whole milk'}\n",
      "0.55 \t {'domestic eggs', 'other vegetables'} -> {'whole milk'}\n",
      "0.52 \t {'yogurt', 'whipped/sour cream'} -> {'whole milk'}\n",
      "0.52 \t {'root vegetables', 'rolls/buns'} -> {'whole milk'}\n",
      "0.52 \t {'pip fruit', 'other vegetables'} -> {'whole milk'}\n"
     ]
    }
   ],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "\n",
    "You can check the results of your implementation using MLXtend. Just run the cell below (you will have to run the checking code of question 1 first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "0.59 \t {'citrus fruit', 'root vegetables'} -> {'other vegetables'}\n",
      "0.58 \t {'root vegetables', 'tropical fruit'} -> {'other vegetables'}\n",
      "0.58 \t {'yogurt', 'curd'} -> {'whole milk'}\n",
      "0.57 \t {'butter', 'other vegetables'} -> {'whole milk'}\n",
      "0.57 \t {'root vegetables', 'tropical fruit'} -> {'whole milk'}\n",
      "0.56 \t {'yogurt', 'root vegetables'} -> {'whole milk'}\n",
      "0.55 \t {'domestic eggs', 'other vegetables'} -> {'whole milk'}\n",
      "0.52 \t {'yogurt', 'whipped/sour cream'} -> {'whole milk'}\n",
      "0.52 \t {'root vegetables', 'rolls/buns'} -> {'whole milk'}\n",
      "0.52 \t {'pip fruit', 'other vegetables'} -> {'whole milk'}\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules as mx_association_rules\n",
    "\n",
    "rules_mx = mx_association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "print_rules_mx(rules_mx,max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPFL Twitter Data\n",
    "\n",
    "Now that we have a working implementation, we will apply the Apriori algorithm on a dataset that you should know pretty well by now: EPFL Twitter data. In that scenario, tweets will be considered as transactions and words will be items. Let's see what kind of frequent associations we can discover.\n",
    "\n",
    "The method below cleans the tweets and formats them in the same format as the transactions of the previous exercise. Run the cells and generate the results for both algorithms. What can you observe from the association rules results? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincentroduit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vincentroduit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize, stem a document\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Remove stop words\n",
    "def clean_voc(documents):\n",
    "    cleaned = []\n",
    "    for tweet in documents:\n",
    "        new_tweet = []\n",
    "        tweet = tokenize(tweet).split()\n",
    "        for word in tweet:\n",
    "            if (word not in stopwords.words('english') and \n",
    "                word not in stopwords.words('german') and\n",
    "                word not in stopwords.words('french')):\n",
    "                if word==\"epflen\":\n",
    "                    word = \"epfl\"\n",
    "                new_tweet.append(word)\n",
    "        if len(new_tweet)>0:\n",
    "            cleaned.append(new_tweet)\n",
    "    return cleaned\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = clean_voc(original_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.08 \t {'epfl', 'via'}\n",
      "0.06 \t {'epfl', 'â€™'}\n",
      "0.05 \t {'epfl', 'new'}\n",
      "0.05 \t {'epfl', 'amp'}\n",
      "0.05 \t {'epfl', 'research'}\n",
      "0.04 \t {'epfl', 'lausann'}\n",
      "0.04 \t {'vdtech', 'epfl'}\n",
      "0.04 \t {'epfl', 'switzerland'}\n",
      "0.04 \t {'epfl', 'robot'}\n",
      "0.03 \t {'epfl', 'day'}\n",
      "0.03 \t {'epfl', 'swiss'}\n",
      "0.03 \t {'vdtech', 'via'}\n",
      "0.03 \t {'vdtech', 'epfl', 'via'}\n",
      "0.03 \t {'epfl', 'scienc'}\n",
      "0.03 \t {'epfl', 'innov'}\n",
      "0.03 \t {'epfl', 'student'}\n",
      "0.03 \t {'epfl', 'first'}\n",
      "0.03 \t {'work', 'epfl'}\n",
      "0.02 \t {'epfl', 'technolog'}\n",
      "0.02 \t {'epfl', '2018'}\n"
     ]
    }
   ],
   "source": [
    "L,support = apriori(documents,min_support = 0.01)\n",
    "print_support(support,20,min_items=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "1.00 \t {'Â«'} -> {'epfl'}\n",
      "1.00 \t {'Â»'} -> {'epfl'}\n",
      "1.00 \t {'Â«'} -> {'Â»'}\n",
      "1.00 \t {'Â»'} -> {'Â«'}\n",
      "1.00 \t {'model'} -> {'epfl'}\n",
      "1.00 \t {'perovskit'} -> {'epfl'}\n",
      "1.00 \t {'next'} -> {'epfl'}\n",
      "1.00 \t {'improv'} -> {'epfl'}\n",
      "1.00 \t {'particip'} -> {'epfl'}\n",
      "1.00 \t {'technolog'} -> {'epfl'}\n",
      "1.00 \t {'drone'} -> {'epfl'}\n",
      "1.00 \t {'epflcampu'} -> {'epfl'}\n",
      "1.00 \t {'learn'} -> {'epfl'}\n",
      "1.00 \t {'present'} -> {'epfl'}\n",
      "1.00 \t {'mooc'} -> {'epfl'}\n",
      "1.00 \t {'show'} -> {'epfl'}\n",
      "1.00 \t {'scientist'} -> {'epfl'}\n",
      "1.00 \t {'brain'} -> {'epfl'}\n",
      "1.00 \t {'eth'} -> {'epfl'}\n",
      "1.00 \t {'Â«'} -> {'epfl', 'Â»'}\n"
     ]
    }
   ],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 3: Pen and Paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given the following accident and weather data. Each line corresponds to one event:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. car_accident rain lightning wind clouds fire\n",
    "2. fire clouds rain lightning wind\n",
    "3. car_accident fire wind\n",
    "4. clouds rain wind\n",
    "5. lightning fire rain clouds  \n",
    "6. clouds wind car_accident  \n",
    "7. rain lightning clouds fire  \n",
    "8. lightning fire car_accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) You would like to know what is the likely cause of all the car accidents. What association rules do you need to look for? Compute the confidence and support values for these rules. Looking at these values, which is the most likely cause of the car accidents?\n",
    "\n",
    "The possible association rules are: \n",
    "- {lightning} â†’ {car accident} support: 0.25 confidence: 0.4 \n",
    "- {wind} â†’ {car accident} support: 0.375 confidence: 0.6 \n",
    "- {fire} â†’ {car accident} support: 0.375 confidence: 0.5 \n",
    "- {clouds} â†’ {car accident} support: 0.25 confidence: 0.33 \n",
    "- {rain} â†’ {car accident} support: 0.125 confidence: 0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Find all the association rules for minimal support 0.6 and minimal confidence of 1.0 (certainty). Follow the apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to find the frequent itemsets. There are five of them:\n",
    "- {clouds} support: 0.75\n",
    "- {wind} support: 0.625\n",
    "- {lightning} support: 0.625\n",
    "- {rain} support: 0.625\n",
    "- {fire} support: 0.75\n",
    "\n",
    "Secondly, we need to select pertinent rules A -> B such that AuB = J and $p(B|A) > c_{min}$. First we construct itemsets of size 2 which appears in the sentences. They are two pairs:\n",
    "- {lightning, fire} support: 0.625\n",
    "- {clouds, rain} support: 0.625\n",
    "\n",
    "From these two sets, we cannot construct a triplet. We can now construct the associate rules:\n",
    "- {lightning} â†’{fire} support: 0.625 confidence: 1.0\n",
    "- {fire} â†’ {lightning} support: 0.625 confidence: 0.833\n",
    "- {clouds} â†’ {rain} support: 0.625 confidence: 0.833\n",
    "- {rain} â†’ {clouds} support: 0.625 confidence: 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
