{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò DIS Midterm - Fall 2022\n",
    "\n",
    "**üéâ Welcome to DIS Midterm that takes place on the 27th of Octomber 2022 from 12:15 to 13:00.**\n",
    "\n",
    "> Please fill the following info:\n",
    "> - Your Name: \n",
    "> - Your SCIPER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° THE BACKSTORY\n",
    "\n",
    "You are an engineer working in the sports news articles website: _allaboutsports.ch_ . The editors' team of this news portal wants to create a piece for the upcoming Summer Olympic Games for the swimming Olympic medalist Katie Ledecky. To do so, they want to collect information not only from the news coverage but also from what is being said about her on social media. Therefore, you are using the Twitter data that have have the hashtag of _#allaboutsportsch_ news portal to find relevant to the athlete tweets that will help the editors.\n",
    "\n",
    "\n",
    "### üê¶ THE DATA\n",
    "\n",
    "The collected tweets are stored into the `allaboutsports_tweets.csv` which contains the following columns:\n",
    "\n",
    "| Column     | Description                   |\n",
    "|------------|-------------------------------|\n",
    "| **id**     | The id of the tweet           |\n",
    "| **tweet**  | The text/content of the tweet |\n",
    "| **relevant**  | Gold labels for retrieval model evaluation in Part 5 |\n",
    "\n",
    "\n",
    "### ‚úÖ THE TASK\n",
    "\n",
    "Build a retrieval system that searches the tweets and retrieve the ones that talk about Katie Ledecky.\n",
    "\n",
    "You will test a Vector Space retrieval model and a Probabilistic model, and you will compare their results & performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer all the parts of the midterm:\n",
    "\n",
    "- [PART 0: Rename your notebook with your SciperNo](#part0)\n",
    "\n",
    "- [PART 1: Parse and understand the data](#part1)\n",
    "    - 1.1 Create the vocabulary of the documents/tweets\n",
    "    - 1.2 Print the 5 most frequent words present in the tweets\n",
    "\n",
    "- [PART 2: Encode documents with Vector Space Retrieval](#part2)\n",
    "    - 2.1 Build the document-frequency matrix.\n",
    "    - 2.2 Build the inverse document-frequency matrix\n",
    "    - 2.3 Vectorize input with Vector Space Model\n",
    "    \n",
    "- [PART 3: Encode documents with Probabilistic Retrieval](#part3)\n",
    "    - 3.1 Compute collection probabilities for each document\n",
    "    - 3.2 Implement query-document likelohood\n",
    "\n",
    "- [PART 4: Retrieve documents](#part4)\n",
    "    - 4.1 Retrieve documents with Vector Space Retrieval \n",
    "    - 4.2 Retrieve documents with Probabilistic retrieval\n",
    "\n",
    "- [PART 5: Evaluate your retrieval system](#part5)\n",
    "    - 5.1 Discuss the precision and recall between the Vector Space and Probabilistic Retrieval\n",
    "    - 5.2 Explain the role of k in retrieval performance\n",
    "    - 5.3 Explain the role of lambda in the Probabilistic retrieval performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçÄ GOOD LUCK üçÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0'></a>\n",
    "## PART 0: Rename your notebook with your SciperNo\n",
    "\n",
    "The final sumbitted file should have the following name: `SciperNo.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "## PART 1: Parse and understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries- you can additionally import any library you want.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "from functools import reduce\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions - Nothing to change here.\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [word.lower() for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "def preprocess_text(documents):\n",
    "    docs = list()\n",
    "    for doc in documents:\n",
    "        docs.append(tokenize(doc))  # tokenize\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "tweets = pd.read_csv('allaboutsports_tweets.csv')\n",
    "\n",
    "# Clean the data\n",
    "tweets['clean_tweet'] = preprocess_text(tweets['tweet'].tolist())\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 1.1 Create the vocabulary of the documents/tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_frequency(documents):\n",
    "    \"\"\"\n",
    "    It parses the input documents and creates a dictionary with the terms and term frequencies.\n",
    "    \n",
    "    INPUT:\n",
    "    Doc1: hello hello world\n",
    "    Doc2: hello friend\n",
    "    \n",
    "    OUTPUT:\n",
    "    {'hello': 2,\n",
    "    'world': 1,\n",
    "    'friend': 1}\n",
    "\n",
    "    :param documents: list of list of str, with the tokenized tweets.\n",
    "    :return: dict, with keys the words and values the frequency of each word.\n",
    "    \"\"\"\n",
    "    vocabulary = dict()\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # --------------\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = get_vocabulary_frequency(tweets['clean_tweet'].tolist())\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 1.2 Print the 5 most frequent words present in the tweets.\n",
    "> Use the vocabulary frequencies you created in the previous question to find the top-5 words/terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "## PART 2: Encode documents with Vector Space Retrieval\n",
    "\n",
    "In this part we will encode/vectorize the documents using the **Vector Space Model**. \n",
    "More specifically:\n",
    "- we will compute the term-frequency matrix **(tf)**\n",
    "- we will compute the inverse document frequency **(idf)**\n",
    "- we will vectorize/encode the tweets with **tf-idf**\n",
    "- we will implement the **cosine similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2.1 Build the term-frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    It creates the term-frequency matrix with rows the terms of the vocabulary and columns the number of documents.\n",
    "    Each value of the matrix represents the frequency (normalized to document max frequecy) of a term (row) \n",
    "    in a document (column).\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    > INPUT:\n",
    "    documents:\n",
    "    Doc1: hello hello world\n",
    "    Doc2: hello friend\n",
    "    \n",
    "    voc: \n",
    "    [hello, world, friend]\n",
    "    \n",
    "    > OUPUT:    \n",
    "    [[1, 1],\n",
    "    [0.5, 0],\n",
    "    [0, 1]]\n",
    "    \n",
    "    :param documents: list of list of str, with the tokenized tweets.\n",
    "    :param vocabulary: dict with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :return: np.array with the document-term frequencies\n",
    "    \"\"\"\n",
    "    document_term_freq = np.zeros(shape=(len(vocabulary), len(documents)))\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    # --------------\n",
    "    \n",
    "    return document_term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = get_tf(tweets['clean_tweet'].tolist(), voc)\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2.2 Build the inverse document-frequency matrix (idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    It computes IDF scores, storing idf values in a dictionary.\n",
    "    \n",
    "    :param documents: list of list of str, with the tokenized tweets.\n",
    "    :param vocabulary: dict with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :return: dict with the terms as keys and values the idf for each term.\n",
    "    \"\"\"\n",
    "    idf = dict()\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    # --------------\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = get_idf(tweets['clean_tweet'].tolist(), voc)\n",
    "len(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf['katie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization of input with the Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_vsr(document, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It takes the input text and vectorizes it based on the tf-idf formula.\n",
    "    \n",
    "    :param document: list of str, with the tokenized tweet\n",
    "    :param vocabulary: dict, with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :param idf: dict, with the terms as keys and values the idf for each term.\n",
    "    :return: np.array, with the vectorized tweet\n",
    "    \"\"\"\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    term_freq = Counter(document)\n",
    "    max_freq = term_freq.most_common(1)[0][1]\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * term_freq[term]/max_freq\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorized_documents = np.array([vectorize_vsr(doc, voc, idf) for doc in tweets['clean_tweet'].tolist()])\n",
    "vectorized_documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity for Vector Space Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"\n",
    "    It computes cosine similarity.\n",
    "    \n",
    "    :param v1: list of floats, with the vector of a document.\n",
    "    :param v2: list of floats, with the vector of a document.\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "        sim = 0\n",
    "    else:\n",
    "        sim = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "## PART 3: Encode documents with Probabilistic Retrieval\n",
    "\n",
    "In this part we will encode/vectorize the documents using the **Probabilistic Model**. \n",
    "More specifically:\n",
    "- we will compute the collection probabilities for each term **($P(t|M_c)$)**\n",
    "- we will copute the query-document likelihood **$P(t_i|document)$** for each query term $t_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.1 Compute collection probabilities $P(t|M_c$) for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_prob(vocabulary):\n",
    "    \"\"\"\n",
    "    Computes the collection probabilities of each term present in the documents/tweets.\n",
    "    \n",
    "    :param vocabulary: dict with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :return: dict with the collection probabilities for each term in the vocabulary.\n",
    "    \"\"\"\n",
    "    probs = dict()\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    # --------------\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_Mc = get_collection_prob(voc)\n",
    "len(p_Mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.2 Implement query-document likelihood $P(q|doc)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_likelihood(query, document, p_Mc, l):\n",
    "    \"\"\"\n",
    "    It computes the probability of a query given a document/tweet.\n",
    "    \n",
    "    :param query: np.array with the tokenized query\n",
    "    :param document: np.array with the tokenized document\n",
    "    :param p_Mc: dict with the collection probabilities for each term in the vocabulary.\n",
    "    :param l: float, smoothing variable lambda.    \n",
    "    :return: float with the query-document likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # --------------\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "## PART 4: Retrieve documents\n",
    "\n",
    "In this part, we will apply both **Vector Space Retrieval** and **Probabilistic retrieval** in order to get the relevant tweets to Katie Ledecky athlete. \n",
    "\n",
    "> There is nothing to implement in this part, however, you should run the following cells and test your implementation in Parts 2&3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"katie\",\"ledecky\",\"athlete\"]\n",
    "doc_ids = tweets['id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieves with Vector Space Retrieval for `query = \"Katie Ledecky\"` and prints the top 10 relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize query and tweets\n",
    "vectorized_documents_vsr =  np.array([vectorize_vsr(doc, voc, idf) for doc in tweets['clean_tweet'].tolist()])\n",
    "vectorized_query_vsr = np.array(vectorize_vsr(query, voc, idf))\n",
    "\n",
    "# performs vector space retrieval\n",
    "scores = dict()\n",
    "for idx, doc_vec in zip(doc_ids, vectorized_documents_vsr):\n",
    "    scores[idx] = cosine_similarity(doc_vec, vectorized_query_vsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 5 retrieved tweets\n",
    "retrieved_tweets_vsr = pd.DataFrame(list(reversed(sorted(scores.items(), key=lambda item: item[1]))),\n",
    "                                    columns=['id', 'score'])\n",
    "retrieved_tweets_vsr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve with Probabilistic Retrieval for `query = \"Katie Ledecky\"` and prints the top 10 relevant along with their probability (score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs probabilistic retrieval\n",
    "probs = dict()\n",
    "for idx, doc in zip(doc_ids, tweets['clean_tweet'].tolist()):\n",
    "    probs[idx] = compute_query_likelihood(query, doc,  p_Mc, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 5 retrieved tweets\n",
    "retrieved_tweets_prob = pd.DataFrame(list(reversed(sorted(probs.items(), key=lambda item: item[1]))),\n",
    "                                     columns=['id', 'score'])\n",
    "retrieved_tweets_prob[:10]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5'></a>\n",
    "## PART 5: Evaluate retrieval\n",
    "\n",
    "In this part, we will evaluate both retrieval systems and discuss the impact of lambda value as well as the number of retrieved documents.\n",
    "\n",
    "We are now going to use the column `'relevant'` in the dataset. It is referring to whether the tweet is relevant to the query or not. We define this column as the 'oracle' that in an ideal scenario knows which tweets are relevant for Katie Ledecky and which are not.\n",
    "Based on this, we will compute the performance of the retrieval systems we built. üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision@k & Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_at_k(retrieved_tweets, gt, k=5):\n",
    "    \"\"\"\n",
    "    It computes the precision score at a defined set of retrieved documents (k).\n",
    "    \n",
    "    :param predict: list of predictions\n",
    "    :param gt: list of actual relevant data\n",
    "    :param k: int\n",
    "    :return: float, the precision at a given k\n",
    "    \"\"\"\n",
    "    results = retrieved_tweets.merge(gt, how=\"outer\", on=\"id\")\n",
    "    return np.array(results[:k]['relevant'].tolist()).mean()\n",
    "\n",
    "def compute_recall_at_k(retrieved_tweets, gt, k=5):\n",
    "    \"\"\"\n",
    "    It computes the recall score at a defined set of retrieved documents (k).\n",
    "    \n",
    "    :param predict: list of predictions\n",
    "    :param gt: list of actual relevant data\n",
    "    :param k: int\n",
    "    :return: float, the precision at a given k\n",
    "    \"\"\"\n",
    "    relevant = len(tweets[tweets['relevant']==1])\n",
    "    results = retrieved_tweets.merge(gt, how=\"outer\", on=\"id\")[:k]\n",
    "    hits = len(results[results['relevant']==1])\n",
    "    return hits / relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 5.1 Discuss the difference in performance between the Vector Space and Probabilistic Retrieval based on the Precision and Recall scores in the provided table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsr_precision = compute_precision_at_k(retrieved_tweets_vsr, tweets[['id', 'relevant']], 5)\n",
    "prob_precision = compute_precision_at_k(retrieved_tweets_prob, tweets[['id', 'relevant']], 5)\n",
    "\n",
    "vsr_recall = compute_recall_at_k(retrieved_tweets_vsr, tweets[['id', 'relevant']], 5)\n",
    "prob_recall = compute_recall_at_k(retrieved_tweets_prob, tweets[['id', 'relevant']], 5)\n",
    "\n",
    "pd.DataFrame([('Vector Space Retrieval', vsr_precision, vsr_recall),\n",
    "              ('Probabilistic Retrieval',prob_precision, prob_recall)], \n",
    "             columns=['Model', 'Precision@5', 'Recall@5']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è PLEASE WRITE YOUR ANSWER HERE**\n",
    "\n",
    "|\n",
    "\n",
    "|\n",
    "\n",
    "|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 5.2 Discuss the following plot regarding the impact of the $k$ variable in the Precision@k metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 5, 10, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Probabilistic Retrieval and evaluate it on different k\n",
    "res = list()\n",
    "for k in ks:\n",
    "    res.append({'Precision': compute_precision_at_k(retrieved_tweets_prob, tweets[['id', 'relevant']], k),\n",
    "                'k': k})\n",
    "res_df = pd.DataFrame(res)\n",
    "sns.relplot(data=res_df, x=\"k\", y=\"Precision\", kind=\"line\").set(title='Precision@k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è PLEASE WRITE YOUR ANSWER HERE**\n",
    "\n",
    "|\n",
    "\n",
    "|\n",
    "\n",
    "|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 5.3 Discuss the following plot regarding the impact of the $\\lambda$ (lambda) in the query-document likelihood probability $P(q|Doc)$.\n",
    "\n",
    "In this example we estimate the likehood for a given query and a given tweet (see below). The $P(t|M_c)$ values are the same as in the rest of the notebook and are calculated based on the whole document collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = [\"katie\",\"ledecky\",\"swimmer\"]\n",
    "a_document= tweets['clean_tweet'].tolist()[3]\n",
    "\n",
    "print('The query: \"{}\"\"'.format(' '.join(new_query)))\n",
    "print('Tweet: \"{}\"'.format(tweets['tweet'].tolist()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each lambda get the query-document likehood estimation.\n",
    "lambdas = [0, 0.25, 0.5, 0.75, 1]\n",
    "probabilities = list()\n",
    "for l in lambdas:\n",
    "    probabilities.append({'Likelihood': compute_query_likelihood(new_query, a_document,  p_Mc, l),\n",
    "                         'lambda': l})\n",
    "\n",
    "res_df = pd.DataFrame(probabilities)\n",
    "print(res_df)\n",
    "sns.relplot(data=res_df, x=\"lambda\", y=\"Likelihood\", \n",
    "            kind=\"line\").set(title='Likelihood of a document on different Lambda values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è PLEASE WRITE YOUR ANSWER HERE**\n",
    "\n",
    "|\n",
    "\n",
    "|\n",
    "\n",
    "|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîö END OF EXAM\n",
    "> Don't forget to change the submitted file with your SciperNo as the file name before submitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "75fd394e35225182f207b93437350142e41aafd8fa2b11cc1a17258e1fa2f196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
