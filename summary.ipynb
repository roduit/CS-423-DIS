{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Information Retrieval Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_docs=documents['document'].str.cat(sep=' ').split(' ')\n",
    "# vocabulary = Counter(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    It creates the term-frequency matrix with rows the terms of the vocabulary and columns the number of documents.\n",
    "    Each value of the matrix represents the frequency (normalized to document max frequecy) of a term (row) \n",
    "    in a document (column).\n",
    "    Example:\n",
    "    \n",
    "    > INPUT:\n",
    "    documents:\n",
    "    Doc1: hello hello world\n",
    "    Doc2: hello friend\n",
    "    \n",
    "    voc: \n",
    "    [hello, world, friend]\n",
    "    \n",
    "    > OUPUT:    \n",
    "    [[1, 1],\n",
    "    [0.5, 0],\n",
    "    [0, 1]]\n",
    "    \n",
    "    :param documents: list of list of str, with the tokenized documents.\n",
    "    :param vocabulary: dict with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :return: np.array with the document-term frequencies\n",
    "    \"\"\"\n",
    "    document_term_freq = np.zeros(shape=(len(vocabulary), len(documents)))\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "    for j, doc in enumerate(documents):\n",
    "        counts = Counter(doc.split(' '))\n",
    "        for i,term in enumerate(vocabulary):\n",
    "            document_term_freq[i][j] = counts[term]\n",
    "    # --------------\n",
    "    \n",
    "    return document_term_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    It computes IDF scores, storing idf values in a dictionary.\n",
    "    \n",
    "    :param documents: list of list of str, with the document text.\n",
    "    :param vocabulary: dict with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :return: dict with the terms as keys and values the idf for each term.\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = np.log(num_documents / np.sum([term in doc for doc in documents]))\n",
    "    # --------------\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tfidf(documents, vocabulary, tf, idf):\n",
    "    \"\"\"\n",
    "    It takes the input text and vectorizes it based on the tf-idf formula.\n",
    "    \n",
    "    :param documents: list of list of str, with the document text.\n",
    "    :param vocabulary: dict, with keys: the terms of all the documents and values: the number of times they appear in the corpus.\n",
    "    :param idf: dict, with the terms as keys and values the idf for each term.\n",
    "    :return: np.array, with the vectorized documents\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "    tfidf_vectors=np.zeros((len(documents),len(vocabulary)))\n",
    "\n",
    "    for doc_id, document in enumerate(documents):\n",
    "        unique_words = set(document.split(' '))\n",
    "        max_count = np.max(tf[:,doc_id])\n",
    "        for word_id, word in enumerate(vocabulary):\n",
    "            if word in unique_words:\n",
    "                tfidf_vectors[doc_id][word_id] = (tf[word_id,doc_id] / max_count) * idf[word]\n",
    "    # --------------\n",
    "    return np.array(tfidf_vectors)\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It generates the vector for an input document (with normalization).\n",
    "    \n",
    "    :param document: list of str with the tokenized documents.\n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param idf: dict with the idf values for each vocabulary word.\n",
    "    :return: list of floats\n",
    "    \"\"\"\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(query)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    word_count = Counter(query)\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = (word_count[term] / max_count) * idf[term]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [['a','a', 'b', 'c'], ['a', 'b'], ['c']]\n",
    "\n",
    "vocabulary = Counter([doc for document in documents for doc in document])\n",
    "\n",
    "documents = ['a a b c', 'a b', 'c']\n",
    "tf = get_tf(documents, vocabulary)\n",
    "idf = get_idf(documents, vocabulary)\n",
    "\n",
    "tfidf = vectorize_tfidf(documents, vocabulary, tf, idf)\n",
    "\n",
    "query = 'a b'\n",
    "vector = vectorize_query(query, vocabulary, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8660254037844388\n",
      "1.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    print(cosine_similarity(vector, tfidf[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocchio's Relevance Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, alpha, beta, gamma):\n",
    "    \"\"\"Expand the query using Rocchio algorithm.\n",
    "\n",
    "    :param relevant_doc_vecs: np.array with the relevant document vectors\n",
    "    :param non_relevant_doc_vecs: np.array with the non relevant document vectors\n",
    "    :param query_vector: np.array with the original query vector\n",
    "    :param alpha: float representing the alpha parameter\n",
    "    :param beta: float representing the beta parameter\n",
    "    :param gamma: float representing the gamma parameter\n",
    "    :return: np.array with the expanded query vector\n",
    "    \"\"\"\n",
    "    \n",
    "    num_rel = len(relevant_doc_vecs)\n",
    "    num_non_rel = len(non_relevant_doc_vecs)\n",
    "    \n",
    "    # Compute the first term in the Rocchio equation\n",
    "    norm_query_vector = query_vector\n",
    "    \n",
    "    # Compute the second term in the Rocchio equation\n",
    "    norm_sum_relevant = 1 / num_rel * np.sum(relevant_doc_vecs, axis=0)\n",
    "    \n",
    "    # Compute the last term in the Rocchio equation\n",
    "    norm_sum_non_relevant = 1 / num_non_rel * np.sum(non_relevant_doc_vecs, axis=0)\n",
    "    \n",
    "    # Sum all the terms\n",
    "    modified_query_vector = alpha * norm_query_vector + beta * norm_sum_relevant - gamma * norm_sum_non_relevant\n",
    "    \n",
    "    # Ignore negative elements\n",
    "    # modified_query_vector = ...\n",
    "    return modified_query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_svd(term_doc_matrix, num_val):\n",
    "    \"\"\"Constructs the truncated SVD of a term-document matrix.\n",
    "    \n",
    "    :param term_doc_matrix: np.array with the term-document matrix\n",
    "    :param num_val: int with the number of singular values to consider\n",
    "    :return: tuple with the truncated SVD matrices\n",
    "    \"\"\"\n",
    "    K, S, Dt = np.linalg.svd(term_doc_matrix, full_matrices=False)\n",
    "\n",
    "    K_sel = K[:,0:num_val]\n",
    "\n",
    "    S_sel = S[:num_val]\n",
    "    S_sel = np.diag(S_sel)\n",
    "\n",
    "    Dt_sel = Dt[0:num_val,:]\n",
    "\n",
    "    return K_sel, S_sel, Dt_sel\n",
    "\n",
    "def construct_query_vector(q, K_s, S_s):\n",
    "    \"\"\"Constructs the query vector using the truncated SVD matrices.\n",
    "\n",
    "    :param q: np.array with the original query vector\n",
    "    :param K_s: np.array with the left singular vectors\n",
    "    :param S_s: np.array\n",
    "\n",
    "    :return: np.array with the query vector\n",
    "    \"\"\"\n",
    "\n",
    "    q_star = np.dot(q, np.dot(K_s, np.linalg.inv(S_s)))\n",
    "    \n",
    "    return q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, given q_star, compute the similarity with the document representation (given by the columns of Dt_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_data_matrix(data,n_users,n_items):\n",
    "    \"\"\"This function should return a numpy matrix with a shape (n_users, n_items). \n",
    "        Each entry is the rating given by the user to the item\n",
    "    \n",
    "    Args:\n",
    "        :data: pandas dataframe with the data\n",
    "        :n_users: number of users\n",
    "        :n_items: number of items\n",
    "\n",
    "    Returns:\n",
    "        :data_matrix: numpy matrix with shape (n_users, n_items)\n",
    "    \"\"\"\n",
    "    data_matrix = np.zeros((n_users, n_items))\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        user_id = row['user_id'] - 1\n",
    "        item_id = row['item_id'] - 1\n",
    "        rating = row['rating']\n",
    "\n",
    "        data_matrix[user_id][item_id] = rating\n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "# item_similarity = cosine_similarity(train_data_matrix.T)\n",
    "# user_similarity = cosine_similarity(train_data_matrix)\n",
    "\n",
    "def item_based_predict(ratings, similarity):\n",
    "    \"\"\"Compute the item-based prediction using the cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        :ratings: numpy matrix with the ratings\n",
    "        :similarity: numpy matrix with the item-item similarity\n",
    "        \n",
    "    Returns:\n",
    "        :pred: numpy matrix with the predicted ratings\n",
    "    \"\"\"\n",
    "    n_users, n_items = ratings.shape\n",
    "\n",
    "    weighted_sum = np.dot(ratings, similarity.T)\n",
    "\n",
    "    sim_sum = np.dot((ratings > 0), np.abs(similarity.T))\n",
    "\n",
    "    pred = np.where(sim_sum > 0, weighted_sum / sim_sum, np.random.randint(1, 6, size=(n_users, n_items)))\n",
    "\n",
    "    return pred\n",
    "\n",
    "def user_based_predict(ratings, similarity):\n",
    "    \"\"\"Compute the user-based prediction using the cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        :ratings: numpy matrix with the ratings\n",
    "        :similarity: numpy matrix with the user-user similarity\n",
    "\n",
    "    Returns:\n",
    "        :pred: numpy matrix with the predicted ratings\n",
    "    \"\"\"\n",
    "    n_users, n_items = ratings.shape  # Infer dimensions dynamically\n",
    "    pred = np.zeros((n_users, n_items))\n",
    "\n",
    "    # Compute user mean ratings, ignoring zero entries\n",
    "    ratings_cpy = ratings.copy()\n",
    "    ratings_cpy[ratings_cpy == 0] = np.nan\n",
    "    usr_mean = np.nanmean(ratings_cpy, axis=1, keepdims=True)  # (n_users, 1)\n",
    "\n",
    "    # Normalize ratings by subtracting user means\n",
    "    norm_ratings = ratings_cpy - usr_mean\n",
    "    norm_ratings = np.nan_to_num(norm_ratings)  # Replace NaNs with 0 for calculations\n",
    "\n",
    "    for i in tqdm(range(n_items)):\n",
    "        ratings_item_i = ratings[:, i]  # All users' ratings for item i\n",
    "        user_rated_i = np.where(ratings_item_i > 0)[0]  # Users who rated item i\n",
    "\n",
    "        if len(user_rated_i) == 0:\n",
    "            pred[:, i] = usr_mean[:, 0]  # If no users rated, fallback to user mean\n",
    "            continue\n",
    "\n",
    "        # Get relevant similarity scores and normalized ratings\n",
    "        sim_matrix = similarity[:, user_rated_i]  # (n_users, |user_rated_i|)\n",
    "        ratings_vector = norm_ratings[user_rated_i, i]  # (|user_rated_i|,)\n",
    "\n",
    "        # Compute prediction using matrix multiplication\n",
    "        numerator = sim_matrix @ ratings_vector  # (n_users,)\n",
    "        denominator = np.sum(np.abs(sim_matrix), axis=1)  # Sum of absolute similarities\n",
    "\n",
    "        # Avoid division by zero\n",
    "        mask = denominator > 0\n",
    "        pred[:, i] = usr_mean[:, 0]  # Start with user mean\n",
    "        pred[mask, i] += numerator[mask] / denominator[mask]  # Add weighted contribution\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(doc_vectors, query_vector, k=10):\n",
    "    \"\"\" It finds the `k` nearest documents to the given query (based on euclidean distance).\n",
    "    :param doc_vectors: An array of document vectors (np.array(np.array)).\n",
    "    :param query_vector: Query representation (np.array)\n",
    "    :return: List of document indices (list(int))\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "    scores = [[euclidean_distance(np.array(query_vector), np.array(doc_vectors[d])), d] for d in range(len(doc_vectors))]\n",
    "    scores.sort(key=lambda x: x[0])\n",
    "    top_k_docs = []\n",
    "    for i in range(k):\n",
    "        top_k_docs.append(scores[i][1])\n",
    "    # --------------\n",
    "    return top_k_docs\n",
    "\n",
    "def knn_weighting_estimate(doc_vectors, doc_labels, query_vector, k=10):\n",
    "    \"\"\" Weighting estimation for kNN classification\n",
    "\n",
    "    Args:\n",
    "        :param doc_vectors: Document vectors (np.array(np.array))\n",
    "        :param doc_labels: Document labels/topics (list)\n",
    "        :param query_vector: Query vector (np.array)\n",
    "        :param k: Number of nearest neighbors to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        :return: A dictionary containing the estimation score for each label/topic (dict)\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "    query_vector = np.array(query_vector)\n",
    "    scores = {}\n",
    "\n",
    "    top_k_docs = knn(doc_vectors, query_vector, k)\n",
    "    distances = np.array([cosine_similarity(query_vector, doc_vectors[doc]) for doc in top_k_docs])\n",
    "\n",
    "    for i in range(min(doc_labels),max(doc_labels)+1):\n",
    "        retrieved_labels = np.array([int(doc_labels[idx] == i) for idx in top_k_docs])\n",
    "\n",
    "        scores[i] = np.dot(distances, retrieved_labels)\n",
    "    # --------------\n",
    "    return scores\n",
    "\n",
    "def knn_probabilistic_estimate(doc_vectors, doc_labels, query_vector, k=10):\n",
    "    \"\"\" Probabilistic estimation for kNN classification\n",
    "\n",
    "    Args:\n",
    "        :param doc_vectors: Document vectors (np.array(np.array))\n",
    "        :param doc_labels: Document labels/topics (list)\n",
    "        :param query_vector: Query vector (np.array)\n",
    "        :param k: Number of nearest neighbors to retrieve\n",
    "    Returns:\n",
    "        :return: A dictionary containing the estimation score for each label/topic (dict)\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "    scores = {}\n",
    "    top_k_docs = knn(doc_vectors, query_vector, k)\n",
    "\n",
    "    for i in range(min(doc_labels),max(doc_labels)+1):\n",
    "        scores[i] = np.sum(np.array([int(doc_labels[idx] == i) for idx in top_k_docs])) / k\n",
    "    # --------------\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocchio's Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocchio_estimate(doc_vectors, doc_labels, query_vector):\n",
    "    \"\"\" \n",
    "    Rocchio classification\n",
    "    :param doc_vectors: Document vectors (np.array(np.array))\n",
    "    :param doc_labels: Document labels/topics (list)\n",
    "    :param query_vector: Query vector (np.array)\n",
    "    \n",
    "    :return: A dictionary containing the estimation score for each label/topic (dict)\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "    doc_labels = np.array(doc_labels).reshape(-1,1)\n",
    "    scores = {}\n",
    "    for i in range(min(doc_labels)[0],max(doc_labels)[0]+1):\n",
    "        nb_docs_class = np.sum(doc_labels == i)\n",
    "        centroid = np.sum((doc_vectors *(doc_labels == i)), axis=0) / nb_docs_class\n",
    "        scores[i] = euclidean_distance(centroid, query_vector)\n",
    "    # --------------\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navie_bayes_classifier(tf_dict, query, topics_probs):\n",
    "    \"\"\" Naive Bayes classification\n",
    "    \n",
    "    Args:\n",
    "        :param tf_dict: A dictionary, with keys the topics/labels and values a dictionary of word frequencies  \n",
    "        :param query: Query vector\n",
    "        :param topics_probs: Probaility distribution of each topic/label (dict)\n",
    "\n",
    "    Returns:\n",
    "        :return: A dictionary containing the log probability estimation for each topic\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "    log_probabilities = {t:0 for t in tf_dict}\n",
    "\n",
    "    for topic in tf_dict:\n",
    "        prob = 0\n",
    "        for word in query:\n",
    "            if word in tf_dict[topic]:\n",
    "                p_wc = (tf_dict[topic][word] + 1) / (sum(tf_dict[t][word] for t in tf_dict) + 1)\n",
    "                prob += np.log(p_wc)\n",
    "        prob += np.log(topics_probs[topic])\n",
    "\n",
    "        log_probabilities[topic] = prob\n",
    "    # --------------\n",
    "    return log_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Link Based Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_eigen(L, R=None):\n",
    "    \"\"\"\n",
    "    Compute the PageRank vector using the eigenvalue method.\n",
    "    \n",
    "    Args:\n",
    "        L: Link matrix\n",
    "        R: Transition probability matrix\n",
    "\n",
    "    Returns:\n",
    "        R: Transition probability matrix\n",
    "        p: PageRank vector\n",
    "    \"\"\"\n",
    "    if R is None:\n",
    "        # Construct transition probability matrix from L\n",
    "        R = L / np.where(L.sum(axis=0) == 0, 1, L.sum(axis=0))\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors of R\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(R)\n",
    "    \n",
    "    # Take the eigenvector corresponding to the largest eigenvalue\n",
    "    dominant_vector = eigenvectors[:, np.argmax(np.absolute(eigenvalues))]\n",
    "    \n",
    "    # Ensure the eigenvector is positive and normalize to sum to 1\n",
    "    p = np.absolute(dominant_vector)\n",
    "    p = p / p.sum()\n",
    "    \n",
    "    return R, p\n",
    "\n",
    "def pagerank_iterative(L, R=None):\n",
    "    \"\"\"Compute the PageRank vector using the iterative method.\n",
    "    \n",
    "    Args:\n",
    "        L: Link matrix\n",
    "        R: Transition probability matrix\n",
    "        \n",
    "    Returns:\n",
    "        R: Transition probability matrix\n",
    "        p: PageRank vector\n",
    "    \"\"\"\n",
    "    if R is None:  # We might want to compute R outside this function to avoid recomputing large matrix\n",
    "        R = np.multiply(L, 1 / np.sum(L,axis=0))\n",
    "\n",
    "    N = L.shape[0]\n",
    "    e = np.ones(shape=(N,1))\n",
    "    q = 0.9\n",
    "\n",
    "    p = e\n",
    "    delta = 1\n",
    "    epsilon = 0.001\n",
    "    i = 0\n",
    "    while delta > epsilon:\n",
    "        p_prev = p\n",
    "        p = q * np.dot(R,p_prev)\n",
    "        p = p + ((1-q) / N) * e\n",
    "        delta = np.linalg.norm(x=(p-p_prev), ord=1)\n",
    "        i += 1\n",
    "\n",
    "    print(\"Converged after {0} iterations. Ranking vector: p={1}\".format(i, p[:,0]))\n",
    "    return R,p\n",
    "\n",
    "def hits_iterative(A, k=10):\n",
    "    \"\"\"Compute the HITS score using the iterative method.\n",
    "\n",
    "    Args:\n",
    "        A: Adjacency matrix\n",
    "        k: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        x: Authority vector\n",
    "        y: Hub vector\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    x0, y0 = 1 / (N**0.5) * np.ones(N), 1 / (N**0.5) * np.ones(N)\n",
    "    xprev, yprev = x0, y0\n",
    "    delta1 = delta2 = 1\n",
    "    epsilon = 0.001 # We can strictly check for convergence rate of HITS algorithm\n",
    "    l = 0\n",
    "    \n",
    "    while l < k and delta1 > epsilon and delta2 > epsilon:\n",
    "        y = np.matmul(A, xprev)\n",
    "        x = np.matmul(np.transpose(A), y) \n",
    "        x = x / np.linalg.norm(x,2)\n",
    "        y = y / np.linalg.norm(y,2)\n",
    "        delta1 = np.linalg.norm(x-xprev,1)\n",
    "        delta2 = np.linalg.norm(y-yprev,1)\n",
    "        xprev = x\n",
    "        yprev = y\n",
    "        l += 1\n",
    "        \n",
    "    return xprev, yprev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Graph Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def compute_modularity(G: nx.Graph) -> float:\n",
    "    \"\"\"Compute the modularity of a graph\n",
    "    \n",
    "    Args:\n",
    "        * G (nx.Graph): the graph to analyze\n",
    "        \n",
    "    Returns:\n",
    "        * float: the modularity of the graph\n",
    "    \"\"\"\n",
    "    m = len(G.edges)\n",
    "    Q = 0\n",
    "    # your code here\n",
    "    for i in range(m-1):\n",
    "        for j in range(m-1):\n",
    "            A_ij = G.number_of_edges(i,j)\n",
    "            k_i = G.degree[i]\n",
    "            k_j = G.degree[j]\n",
    "            if G.nodes[i]['community'] == G.nodes[j]['community']:\n",
    "                Q += (A_ij - (k_i * k_j) / (2*m))\n",
    "    return Q/(2*m)\n",
    "\n",
    "def compute_betweenness(G: nx.Graph) -> dict:\n",
    "    \"\"\"Compute the betweenness centrality of a graph.\n",
    "    \n",
    "    Args:\n",
    "        * G (nx.Graph): the graph to analyze\n",
    "        \n",
    "    Returns:\n",
    "        * dict: the betweenness centrality of each node\n",
    "    \"\"\"\n",
    "\n",
    "    betweenness_centralities = {n: 0.0 for n in G.nodes()}\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    for n in nodes:\n",
    "        for i in range(len(nodes)):\n",
    "            s = nodes[i]\n",
    "            for j in range(i + 1, len(nodes)):  # Ensure i < j to avoid redundant pairs\n",
    "                t = nodes[j]\n",
    "\n",
    "                paths = list(nx.all_shortest_paths(G, source=s, target=t))  # Convert to list once\n",
    "                num_paths = sum(1 for path in paths if n in path and n not in {s, t})  # Count only valid paths\n",
    "                \n",
    "                if paths:  # Avoid division by zero\n",
    "                    betweenness_centralities[n] += num_paths / len(paths)\n",
    "\n",
    "    return betweenness_centralities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    It computes cosine similarity.\n",
    "    \n",
    "    :param v1: list of floats, with the vector of a document.\n",
    "    :param v2: list of floats, with the vector of a document.\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "        sim = 0\n",
    "    else:\n",
    "        sim = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(v1, v2):\n",
    "    \"\"\" It computes the euclidean distance between to vectors.\n",
    "    :param v1: First vector (numpy array).\n",
    "    :param v2: Second vector (numpy array).\n",
    "    :return: Euclidean distance (float)\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(v1 - v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(predict, gt, k):\n",
    "    \"\"\"\n",
    "    It computes the recall score at a defined set of retrieved documents.\n",
    "    \n",
    "    :param predict: list of predictions\n",
    "    :param gt: list of actual data\n",
    "    :param k: int\n",
    "    \"\"\"\n",
    "    return len(set(predict[:k]).intersection(set(gt))) / len(gt)\n",
    "\n",
    "def compute_precision_at_k(predict, gt, k):\n",
    "    \"\"\"\n",
    "    It computes the precision score at a defined set of retrieved documents.\n",
    "    \n",
    "    :param predict: list of predictions\n",
    "    :param gt: list of actual data\n",
    "    :param k: int\n",
    "    \"\"\"\n",
    "    return len(set(predict[:k]).intersection(set(gt))) / k\n",
    "\n",
    "def compute_map(queries, K = 10):\n",
    "    \"\"\"\n",
    "    It computes mean average precision (MAP) for a set of queries.\n",
    "    \n",
    "    :param queries: list of str\n",
    "    :param K: int\n",
    "    \"\"\"\n",
    "    map_score = 0\n",
    "    for i, query in enumerate(queries):\n",
    "        precision_for_query = 0\n",
    "        predict = search_vec(query, K)\n",
    "        gt = search_vec_sklearn(query, features)\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        for k in range(1, K+1):\n",
    "            precisions.append(compute_precision_at_k(predict, gt, k))\n",
    "            recalls.append(compute_recall_at_k(predict, gt, k))\n",
    "        interpolate = [max(precisions[i:]) for i in range(len(precisions))]\n",
    "        retrieved_idx = np.array([pred in gt for pred in predict])\n",
    "\n",
    "        interpolate = interpolate * retrieved_idx\n",
    "\n",
    "        map_score += 1 / len(gt) * np.sum(interpolate)\n",
    "    map_score = 1 / len(queries) * map_score\n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(k, scores):\n",
    "    \"\"\"Compute DCG@k for a given list of scores.\n",
    "    \n",
    "    :param k: int\n",
    "    :param scores: list of floats\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    dcg_val = 0\n",
    "    for i in range(1, k):\n",
    "        dcg_val += scores[i] / math.log2(i+1)\n",
    "    return dcg_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(prediction, ground_truth):\n",
    "    \"\"\" Compute the Root Mean Squared Error of the prediction.\n",
    "    Args:\n",
    "        :param prediction: np.array with the predicted values\n",
    "        :param ground_truth: np.array\n",
    "    \n",
    "    Returns:\n",
    "        :return: float\n",
    "    \"\"\"\n",
    "    mask = ground_truth > 0\n",
    "    N = np.sum(mask)\n",
    "\n",
    "    prediction = prediction * mask\n",
    "\n",
    "    squares = np.sqrt(1/N * np.sum((prediction - ground_truth)**2))\n",
    "    return squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def pearson_correlation(vec1, vec2):\n",
    "    \"\"\" Compute the Pearson correlation between two vectors.\n",
    "    Args:\n",
    "        :param vec1: np.array with the first vector\n",
    "        :param vec2: np.array with the second vector\n",
    "    \n",
    "    Returns:\n",
    "        :return: float\n",
    "    \"\"\"\n",
    "    mean1 = np.mean(vec1)\n",
    "    mean2 = np.mean(vec2)\n",
    "\n",
    "    num = np.sum((vec1 - mean1) * (vec2 - mean2))\n",
    "    den = np.sqrt(np.sum((vec1 - mean1)**2) * np.sum((vec2 - mean2)**2))\n",
    "\n",
    "    return num / den\n",
    "\n",
    "vec1 = [1,3]\n",
    "vec2 = [1,4]\n",
    "\n",
    "print(pearson_correlation(vec1, vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the number max of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 10\n",
    "n_edges_max = int(n_nodes * (n_nodes-1) / 2)\n",
    "print(n_edges_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
